# ğŸ“£ Overview

## MorphoBench

![MorphoBench Overview](./asset/MorphoBench.jpg)

MorphoBench is an adaptive reasoning benchmark for large-scale models. It curates over 1,300 multidisciplinary questions and dynamically adjusts task difficulty based on model reasoning traces, providing a scalable and reliable framework for evaluating the reasoning performance of advanced models like o3 and GPT-5.

# ğŸ“ Dataset

The MorphoBench dataset is available on Hugging Face: [OpenDCAI/MorphoBench](https://huggingface.co/datasets/OpenDCAI/MorphoBench)

```python
from datasets import load_dataset
dataset = load_dataset("OpenDCAI/MorphoBench")
```

After downloading, create a data/ folder inside your local project directory and place the datasets there:
ã€ã€ã€
MorphoBench/
â”œâ”€â”€ adaption/
â”œâ”€â”€ asset/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Morpho_P_Perturbed/
â”‚   â”œâ”€â”€ Morpho_P_v0/
â”‚   â”œâ”€â”€ Morpho_R_Complex/
â”‚   â”œâ”€â”€ Morpho_R_Lite/
â”‚   â””â”€â”€ Morpho_R_v0/
â”œâ”€â”€ scripts/
â”œâ”€â”€ output/
â””â”€â”€ ...
ã€ã€ã€
# âš™ï¸ Usage

## Environment Setup

ã€ã€ã€bash
cd Morphobench
pip install -r requirements.txt
ã€ã€ã€

## Run Inference

Generate model predictions for all datasets:
ã€ã€ã€bash
bash scripts/run_batch.sh
ã€ã€ã€

Predictions will be saved under:
ã€ã€ã€
output/infer_result/
ã€ã€ã€

## Evaluate Results

Evaluate the reasoning performance:
ã€ã€ã€bash
bash scripts/evaluate_batch.sh
ã€ã€ã€

Evaluation metrics will be stored in:
ã€ã€ã€
output/eval_result/
ã€ã€ã€
# ğŸ™ Acknowledgements

This project adapts evaluation script logic from [Humanity's Last Exam](https://github.com/centerforaisafety/hle).

